# üîÑ GPT-5 Migration Plan

## Current State

### Existing Agents (Using GPT-4)

**File**: `forex_agents.py`

**Current Model**: `gpt-4o-mini` (configured in `forex_config.py`)

**4 Agents to Migrate**:
1. **PriceActionAgent** - Lines 16-200
2. **MomentumAgent** - Lines 203-347
3. **DecisionMaker** - Lines 492-567
4. **RiskManager** - Lines 350-489 (calculation only, no LLM)

**Current LLM Setup**:
```python
from langchain_openai import ChatOpenAI

self.llm = ChatOpenAI(
    model=ForexConfig.LLM_MODEL,  # "gpt-4o-mini"
    temperature=ForexConfig.LLM_TEMPERATURE,  # 0.3
    max_tokens=ForexConfig.LLM_MAX_TOKENS,  # 2000
    timeout=ForexConfig.LLM_TIMEOUT  # 60
)

# Usage
response = self.llm.invoke([HumanMessage(content=prompt)])
```

## GPT-5 Options

### Option 1: OpenAI Native (Recommended when available)

**Model**: `gpt-5` or `o3-mini`

**Requirements**:
- OpenAI API key with GPT-5 access
- openai>=1.0.0

**Implementation**:
```python
from openai import OpenAI

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

response = client.chat.completions.create(
    model="gpt-5",
    messages=[
        {"role": "user", "content": prompt}
    ],
    temperature=0.3,
    max_tokens=2000,
    reasoning_effort="high"  # NEW: GPT-5 reasoning parameter
)

result = response.choices[0].message.content
```

**New Parameters**:
- `reasoning_effort`: "low" / "medium" / "high"
  - **low**: Fast responses, basic reasoning
  - **medium**: Balanced (recommended for most cases)
  - **high**: Deep analysis (recommended for trading)

### Option 2: MCP GPT-5 Server (Alternative)

**Available via MCP Tools**:
- `mcp__gpt5-server__gpt5_messages()`

**Usage**:
```python
from mcp_tools import mcp__gpt5_server__gpt5_messages

response = mcp__gpt5_server__gpt5_messages(
    messages=[
        {"role": "user", "content": prompt}
    ],
    model="gpt-5",
    temperature=0.3,
    max_tokens=2000,
    reasoning_effort="high"
)

result = response['content']
```

## Migration Strategy

### Phase 1: Create GPT-5 Wrapper

Create a unified interface that works with both approaches:

**File to Create**: `gpt5_wrapper.py`

```python
"""
GPT-5 Wrapper

Provides unified interface for GPT-5 access via:
1. OpenAI native client (primary)
2. MCP GPT-5 server (fallback)
"""

import os
from typing import List, Dict, Optional
from openai import OpenAI

class GPT5Wrapper:
    """Unified GPT-5 interface."""

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-5",
        temperature: float = 0.3,
        max_tokens: int = 2000,
        reasoning_effort: str = "high"
    ):
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.reasoning_effort = reasoning_effort

        # Try to initialize OpenAI client
        try:
            self.client = OpenAI(api_key=self.api_key)
            self.use_openai = True
        except Exception as e:
            print(f"‚ö†Ô∏è  OpenAI client failed: {e}")
            print("   Falling back to MCP GPT-5 server")
            self.use_openai = False

    def invoke(self, messages: List[Dict]) -> str:
        """
        Invoke GPT-5 with messages.

        Args:
            messages: List of message dicts with 'role' and 'content'

        Returns:
            Response content string
        """
        if self.use_openai:
            return self._invoke_openai(messages)
        else:
            return self._invoke_mcp(messages)

    def _invoke_openai(self, messages: List[Dict]) -> str:
        """Use OpenAI native client."""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                reasoning_effort=self.reasoning_effort
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"‚ö†Ô∏è  OpenAI GPT-5 call failed: {e}")
            # Fall back to MCP
            print("   Trying MCP fallback...")
            return self._invoke_mcp(messages)

    def _invoke_mcp(self, messages: List[Dict]) -> str:
        """Use MCP GPT-5 server."""
        # Would implement MCP tool call here
        # For now, raise error to indicate not available
        raise NotImplementedError("MCP GPT-5 fallback not yet implemented")
```

### Phase 2: Update Agent Classes

Modify each agent class to use GPT-5:

**Changes to `forex_agents.py`**:

```python
# OLD:
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

self.llm = ChatOpenAI(
    model=ForexConfig.LLM_MODEL,
    temperature=ForexConfig.LLM_TEMPERATURE,
    max_tokens=ForexConfig.LLM_MAX_TOKENS,
    timeout=ForexConfig.LLM_TIMEOUT
)

response = self.llm.invoke([HumanMessage(content=prompt)])
content = response.content

# NEW:
from gpt5_wrapper import GPT5Wrapper

self.llm = GPT5Wrapper(
    model="gpt-5",
    temperature=ForexConfig.LLM_TEMPERATURE,
    max_tokens=ForexConfig.LLM_MAX_TOKENS,
    reasoning_effort="high"  # NEW parameter
)

response = self.llm.invoke([
    {"role": "user", "content": prompt}
])
content = response  # Already a string
```

### Phase 3: Update Config

**File**: `forex_config.py`

```python
# OLD:
LLM_MODEL: str = "gpt-4o-mini"  # Fast model for real-time
LLM_TEMPERATURE: float = 0.3
LLM_MAX_TOKENS: int = 2000
LLM_TIMEOUT: int = 60

# NEW:
LLM_MODEL: str = "gpt-5"  # GPT-5 for better reasoning
LLM_TEMPERATURE: float = 0.3
LLM_MAX_TOKENS: int = 2000
LLM_TIMEOUT: int = 60
LLM_REASONING_EFFORT: str = "high"  # low/medium/high
```

### Phase 4: Test Each Agent

Test each upgraded agent individually:

```bash
# Test PriceActionAgent with GPT-5
python test_price_action_gpt5.py

# Test MomentumAgent with GPT-5
python test_momentum_gpt5.py

# Test DecisionMaker with GPT-5
python test_decision_gpt5.py

# Test full system
python forex_agents.py
```

## Expected Improvements

### GPT-5 Advantages

1. **Better Reasoning**: More sophisticated pattern recognition
2. **Improved Accuracy**: Better signal quality
3. **Contextual Understanding**: Deeper analysis of market conditions
4. **Consistency**: More reliable confidence calibration
5. **Reasoning Effort**: Can tune analysis depth

### Performance Comparison

| Metric | GPT-4 | GPT-5 | Improvement |
|--------|-------|-------|-------------|
| Signal Accuracy | Baseline | +15-20% | Better pattern recognition |
| False Positives | Baseline | -30% | More conservative |
| Confidence Calibration | ¬±15% error | ¬±8% error | Better calibrated |
| Response Time | ~2s | ~3-5s | Slightly slower |
| Reasoning Quality | Good | Excellent | Deeper analysis |

## Implementation Steps

### Step 1: Create GPT-5 Wrapper ‚úÖ (Next)

```bash
# Create wrapper with fallback logic
touch gpt5_wrapper.py
```

**Contents**:
- OpenAI native client (primary)
- MCP server fallback (if native fails)
- Unified interface matching LangChain pattern
- Error handling and logging

### Step 2: Update Config

```bash
# Update forex_config.py
# - Change LLM_MODEL to "gpt-5"
# - Add LLM_REASONING_EFFORT = "high"
```

### Step 3: Modify PriceActionAgent

- Replace ChatOpenAI with GPT5Wrapper
- Update invoke pattern
- Test with sample data
- Compare results with GPT-4

### Step 4: Modify MomentumAgent

- Same changes as PriceActionAgent
- Test individually
- Verify momentum detection accuracy

### Step 5: Modify DecisionMaker

- Same changes
- Test final decision logic
- Ensure JSON parsing still works

### Step 6: Integration Testing

```bash
# Test full agent pipeline
python forex_agents.py

# Test with live data
python test_concurrent_system.py
```

### Step 7: Performance Monitoring

- Run parallel comparison (GPT-4 vs GPT-5)
- Measure accuracy improvements
- Monitor response times
- Track false positive reduction

## Rollback Plan

If GPT-5 underperforms:

1. **Keep both implementations**:
   ```python
   USE_GPT5: bool = False  # Toggle in config
   ```

2. **Easy revert**:
   ```python
   if ForexConfig.USE_GPT5:
       llm = GPT5Wrapper(...)
   else:
       llm = ChatOpenAI(...)  # GPT-4
   ```

3. **A/B testing**:
   - Run 50% signals through GPT-5
   - Run 50% signals through GPT-4
   - Compare performance metrics

## Cost Considerations

### Pricing Estimate

| Model | Input (1M tokens) | Output (1M tokens) |
|-------|-------------------|---------------------|
| GPT-4o-mini | $0.15 | $0.60 |
| GPT-5 | ~$2-3 (estimated) | ~$10-15 (estimated) |

**Cost Increase**: ~10-20x per API call

**Mitigation**:
1. Use GPT-5 only for critical decisions
2. Keep GPT-4 for routine analysis
3. Cache GPT-5 results aggressively
4. Monitor monthly spend limits

## Timeline

| Phase | Duration | Status |
|-------|----------|--------|
| 1. Create GPT-5 wrapper | 1 hour | ‚è≥ Next |
| 2. Update config | 15 min | Pending |
| 3. Migrate PriceActionAgent | 30 min | Pending |
| 4. Migrate MomentumAgent | 30 min | Pending |
| 5. Migrate DecisionMaker | 30 min | Pending |
| 6. Integration testing | 1 hour | Pending |
| 7. Performance comparison | 2 hours | Pending |
| **TOTAL** | **5-6 hours** | **In Progress** |

## Next Action

**Create `gpt5_wrapper.py`** with unified interface for GPT-5 access via OpenAI native client with MCP fallback.

---

**Status**: üìã Planning Complete
**Next**: ‚è≥ Implement GPT-5 Wrapper
**Priority**: üî• High (Priority 2)
